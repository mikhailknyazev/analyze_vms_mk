"""
VM Migration Analysis Script from RVTools Export. Version 1.1.

MIT License
Copyright (c) 2025 Michael Knyazev (Mikhail Vladimirovich Kniazev)

Author: Michael Knyazev
- https://www.linkedin.com/in/mikhailkniazev/
- https://github.com/mikhailknyazev

DISCLAIMER:
This script is provided as-is and may be outdated or incomplete.
You are encouraged to modify or extend it to suit your specific environment and requirements.
The generated risk score is a heuristic and may not capture all complexities of individual VMs, migration scenarios,
or the latest changes in target technologies. Always validate the results against detailed VM data,
project-specific needs, and the most recent official documentation from Red Hat and other relevant vendors.

Purpose:
This script processes an RVTools export (specifically a single CSV file containing
multiple 'sheets' or tables) to analyze Virtual Machines (VMs) for migration
readiness, particularly focusing on aspects relevant for migrations to platforms
like OpenShift Virtualization (using MTV - Migration Toolkit for Virtualization).
It generates a text-based report summarizing VM characteristics, potential
migration complexities, and a heuristic risk score.

Expected RVTools Input File:
  - Format: A single CSV file generated by RVTools (tested with versions like 4.x).
  - Encoding: The script attempts to read with 'utf-8-sig' (UTF-8 with BOM),
    then 'utf-8'.
  - Structure: The CSV contains multiple data tables, each preceded by a
    marker line like "vInfo: table 1", "vDisk: table 1", etc.
  - Key 'Sheets' Used:
    - "vInfo": Primary VM data (VM name, power state, OS, CPU, memory,
               HW version, annotations, firmware, Change Version, etc.).
               Expected columns are listed in `VINFO_COLS_ACTUAL`.
    - "vDisk": VM disk information (capacity, thin provisioning, controller type).
               Expected columns are listed in `VDISK_COLS_ACTUAL`.
    - "vNetwork": VM Network Interface Card (NIC) details (adapter type, network name).
                  Expected columns are listed in `VNETWORK_NIC_COLS_ACTUAL`.
    - "vSnapshot": VM snapshot details (name, date, size).
                   Expected columns are listed in `VSNAPSHOT_COLS_ACTUAL`.
    - "vTools": VMware Tools status and version from a dedicated sheet, used to
                corroborate/enhance data from vInfo.
                Expected columns are listed in `VTOOLS_COLS_ACTUAL`.
  - Column Headers: The script expects specific column names (defined in
    `VINFO_COLS_ACTUAL`, `VDISK_COLS_ACTUAL`, etc.). Column names are
    sanitized by removing characters like '<>/\\:'.

OS Breakdown Configuration File (OS_BREAKDOWN_FILE_PATH):
  - Default Name: 'OS Breakdown-Table 1.csv' (Place in the same directory as the script or provide path)
  - Format: CSV file.
  - Purpose: Maps OS strings reported by RVTools to a standardized OS name
    and a "complexity" profile (e.g., Easy, Medium, Hard, Windows Server,
    Linux, Recreate, Retire). This mapping is crucial for risk scoring.
  - Expected Columns (header row, case-insensitive, spaces become underscores):
    1. 'os_reported_in_rvtools': The exact OS string as found in the
       RVTools 'OS according to the VMware Tools' or
       'OS according to the configuration file' columns.
    2. 'os_mapped_name': A standardized or canonical name for the OS
       (e.g., "Windows Server 2019 Datacenter", "RHEL 8.x").
    3. 'os_complexity': A string indicating the perceived migration
       complexity or type (e.g., "Easy", "Medium", "Hard", "Windows Server",
       "Linux", "Recreate", "Retire", "Legacy Unix", "Unknown").
       This directly influences the risk score.

EXCLUSION_REGEX_PATTERNS:
  - Purpose: This is a list of regular expression patterns.
  - Usage: Each VM's name (from the 'VM' column in the vInfo sheet) is
    checked against these patterns using a full match (case-insensitive).
  - Effect: If a VM name matches any pattern, it is flagged as an "excluded VM".
    Excluded VMs are typically infrastructure-related and are
    not included in the detailed migration risk analysis or scoring. They are
    summarized in a separate section of the report.

Output Report File (Text):
  - Format: Plain text (.txt).
  - Naming: Defaults to "<rvtools_filename>__migration_focused_report.txt"
    or can be specified with the --output_fpath argument.
  - Content Sections:
    1. Header: Source file names, VI SDK API Version (from first VM in vInfo),
       generation timestamp.
    2. Important Notes & Manual Checks: General advice for migrations.
    3. Overall Summary:
       - Total VMs from RVTools.
       - Counts of detected templates, excluded infrastructure VMs, and
         migration candidates.
    4. Summary for Migration Candidates:
       - Counts of powered on/off VMs.
       - Aggregated vCPUs, Memory, Provisioned Storage (vInfo),
         In-Use Storage (vInfo), and Total Disk Capacity (vDisk sum).
    5. Calculated Risk Score Explanation: Details how the heuristic risk
       score is derived from various VM attributes (OS profile, VMware Tools,
       HW version, disk controllers, network adapters, snapshots, firmware,
       passthrough devices).
    6. VM Count by Risk Score: A breakdown of how many migration candidate
       VMs fall into each calculated risk score.
    7. Detected Templates: A list of VMs identified as templates, with basic information.
    8. VMs Excluded by Name Pattern: A list of infrastructure/special VMs excluded
       from detailed analysis.
    9. Detailed VM Information & Migration Considerations:
       - Grouped by "Migration Recommendation Category" (e.g., LOW COMPLEXITY,
         MEDIUM COMPLEXITY, REVIEW / REHOST / REPLATFORM / REFACTOR).
       - For each migration candidate VM:
         - VM Name
         - Calculated Risk Score and breakdown of contributing factors.
         - Mapped OS Name and OS Profile.
         - Storage summary (number of disks, max disk size).
         - Annotation (functional purpose).
         - Detailed Attributes: Power state, raw OS string, CPUs, memory,
           storage figures, VMware Tools status/version, HW version, cluster,
           resource pool, folder path, NIC count, firmware, disk controllers,
           network adapters, snapshots, passthrough devices, CBT status,
           hibernation/suspend status.
         - Migration Considerations: Primary risk factors, notes on tools,
           HW version, firmware, disk controllers, NICs, snapshots,
           passthrough, CBT.

Output Report File (CSV):
  - Format: Comma Separated Values (.csv).
  - Naming: Defaults to "<rvtools_filename>__migration_focused_report.csv"
    or can be specified with the --output_csv_fpath argument.
  - Content: A flat table with one row per VM (including templates and excluded VMs)
    and detailed attributes and migration considerations as columns.
"""

import pandas as pd
import re
from collections import defaultdict, Counter
import sys
import argparse
import os
import csv
from io import StringIO
import datetime
import math # Used for math.isnan

# --- Configuration & Constants ---
OS_BREAKDOWN_FILE_PATH = 'OS Breakdown-Table 1.csv'

# REGEX patterns for excluding VMs (e.g., infrastructure, vCenter, NSX)
# Users should customize this list based on their environment's naming conventions.
# The script will perform a case-insensitive full match against the VM name.
EXCLUSION_REGEX_PATTERNS = [
    re.compile(r"core-a-.+", re.IGNORECASE),         # Example category A
    re.compile(r"infra-(group-)?[xyz]\d*", re.IGNORECASE), # Example category B
    re.compile(r"net-(group-)?[xyz]\d*", re.IGNORECASE),    # Example category C
    re.compile(r"display-.+", re.IGNORECASE)       # Example category D
]

# Defines the expected column names in the 'vInfo' sheet of the RVTools export.
# Order matters for some initial parsing steps.
VINFO_COLS_ACTUAL = ['VM', 'Powerstate', 'Template',
                     'OS according to the VMware Tools',
                     'OS according to the configuration file',
                     'CPUs', 'Memory',
                     'Provisioned MiB',
                     'In Use MiB',
                     'Disks', 'NICs',
                     'VMware Tools', 'VMware Tools Version',
                     'HW version', 'Cluster',
                     'Resource pool', 'Folder',
                     'Annotation', 'Firmware',
                     'Change Version', 'Fixed Passthru HotPlug', # For passthrough device detection
                     'Suspended To Memory', 'Suspend time',      # For hibernation/suspend check
                     'VI SDK API Version'                       # For vSphere version context
                     ]

# Defines expected columns for other relevant RVTools sheets.
VDISK_COLS_ACTUAL = ['VM', 'Disk Path', 'Capacity MiB', 'Thin', 'Controller']
VNETWORK_NIC_COLS_ACTUAL = ['VM', 'NIC label', 'Network', 'Adapter', 'Type',
                            'Mac Address', 'IPv4 Address', 'Connected']
VSNAPSHOT_COLS_ACTUAL = ['VM', 'Name', 'Description', 'Date / time',
                         'Size MiB (vmsn)', 'State']
VTOOLS_COLS_ACTUAL = ['VM', 'Tools Version', 'Tools'] # From the vTools sheet

# Base names of sheets the script might encounter or try to parse markers for.
# Used to identify transitions between sheets in the single CSV file.
BASE_SHEET_NAMES_FOR_TABLE_1_PATTERN = [
    "vinfo", "vcpu", "vmemory", "vdisk", "vpartition", "vnetwork",
    "vsnapshot", "vtools", "vpowerstate", "vhost", "vcluster", "vrespool", "vdatastore",
    "vhealth", "vlicense", "vdistributedswitch", "vdistributedportgroup", "vapp"
]

# Disk controllers generally considered compatible with MTV or standard migration tools.
MTV_COMPATIBLE_DISK_CONTROLLERS = [
    'VMware Paravirtual', 'VirtIO', 'PVSCSI', 'LSI Logic SAS', 'LSI Logic Parallel', 'SCSI'
]
# Network adapters preferred for performance and compatibility, especially with KVM/MTV.
MTV_PREFERRED_NETWORK_ADAPTERS = ['VMXNET3', 'VirtIO']
# NVMe controllers are explicitly handled as problematic for direct MTV migration
# and require special attention or conversion.
MTV_UNSUPPORTED_DISK_CONTROLLERS = ['NVMe controller', 'NVME controller']


# --- Helper Functions ---
def sanitize_column_name(name):
    """Removes potentially problematic characters from column names."""
    if isinstance(name, str):
        return re.sub(r'[<>/\\:]', '', name).strip()
    return name

def robust_to_numeric(series, default=0.0):
    """
    Converts a pandas Series to numeric, handling errors and providing a default.
    It attempts to remove thousands separators (commas) before conversion.
    """
    # Attempt to replace commas and convert, coercing errors to NaN
    numeric_series = pd.to_numeric(series.astype(str).str.replace(',', '', regex=False), errors='coerce')
    # Fill NaN values resulting from coercion or original NaNs with the specified default
    return numeric_series.fillna(default)

def read_rvtools_sheet(file_path, sheet_name_base, expected_cols):
    """
    Reads a specific 'sheet' (data table) from the RVTools single-file CSV export.

    The RVTools CSV format contains multiple tables. This function locates a
    specific table by a marker (e.g., "vInfo: table 1") and then reads its
    header and data rows until the next table marker or end of file is encountered.

    For the 'vInfo' sheet, data is parsed line-by-line due to potential complexities
    and to ensure specific data type conversions. For other sheets, pandas.read_csv
    is used for efficiency after extracting the relevant text block.

    Args:
        file_path (str): Path to the RVTools CSV file.
        sheet_name_base (str): The base name of the sheet to read (e.g., "vInfo").
        expected_cols (list): A list of expected column names for the target sheet.

    Returns:
        pandas.DataFrame or list: For 'vInfo', returns a list of dictionaries.
                                  For other sheets, returns a pandas DataFrame.
                                  Returns an empty structure if the sheet/file
                                  is not found or an error occurs.
    """
    print(f"Attempting to read sheet: '{sheet_name_base}' from {file_path}")
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            lines = f.readlines()
    except FileNotFoundError:
        print(f"Error: File not found: {file_path}"); return pd.DataFrame(columns=expected_cols) if sheet_name_base != 'vInfo' else []
    except Exception as e:
        print(f"Error opening file {file_path}: {e}"); return pd.DataFrame(columns=expected_cols) if sheet_name_base != 'vInfo' else []

    header_line_idx = -1
    header_content_for_parsing = None
    sheet_marker_to_find = f"{sheet_name_base.lower()}: table 1"

    # Strategy 1: Find the sheet marker (e.g., "vinfo: table 1")
    for i, line_text in enumerate(lines):
        if line_text.strip().lower() == sheet_marker_to_find:
            if i + 1 < len(lines) and lines[i+1].strip(): # Header should be on the next line
                header_line_idx = i + 1
                header_content_for_parsing = lines[header_line_idx].strip()
                # print(f"  Found sheet marker '{lines[i].strip()}' at line {i+1}. Header expected at line {header_line_idx+1}.")
                break
            else:
                # Marker found, but no subsequent line for header
                print(f"  Found sheet marker '{lines[i].strip()}' at line {i+1}, but no subsequent line for header for '{sheet_name_base}'.")
                return pd.DataFrame(columns=expected_cols) if sheet_name_base != 'vInfo' else []

    # Strategy 2: If marker not found, try to find header by content match (first few expected columns)
    if header_line_idx == -1:
        # print(f"  Marker '{sheet_name_base}' not found. Trying content-based header search for '{sheet_name_base}'.")
        # Use the first few expected columns (sanitized) to identify a potential header line
        sanitized_expected_prefix = [sanitize_column_name(ec) for ec in expected_cols[:min(3, len(expected_cols))]]
        for i, line_text in enumerate(lines):
            if not line_text.strip(): continue # Skip empty lines
            try:
                potential_header_parts = [sanitize_column_name(h) for h in line_text.split(',')]
                is_ordered_match = True
                if len(potential_header_parts) >= len(sanitized_expected_prefix):
                    for idx, sech_col in enumerate(sanitized_expected_prefix):
                        if sech_col.lower() != potential_header_parts[idx].lower():
                            is_ordered_match = False
                            break
                else:
                    is_ordered_match = False

                if is_ordered_match:
                    header_line_idx = i
                    header_content_for_parsing = lines[header_line_idx].strip()
                    # print(f"  Found potential header by content match for '{sheet_name_base}' at line {header_line_idx+1}.")
                    break
            except Exception: # If line_text.split(',') fails or other unexpected issue
                continue # Try next line

    if header_line_idx == -1 or header_content_for_parsing is None:
        print(f"Error: Could not find header for sheet '{sheet_name_base}'.")
        return pd.DataFrame(columns=expected_cols) if sheet_name_base != 'vInfo' else []

    data_lines_for_parser = []
    start_data_idx = header_line_idx + 1

    # Extract data lines for the current sheet
    # Stop if a blank line is followed by another sheet's marker, or if a different sheet marker is found.
    for i in range(start_data_idx, len(lines)):
        current_line_raw = lines[i]
        current_line_stripped = current_line_raw.strip()

        if not current_line_stripped: # Potential end of current sheet's data (blank line)
            if i + 1 < len(lines):
                next_line_stripped_lower = lines[i+1].strip().lower()
                # Check if the next non-blank line is a marker for any known sheet
                is_next_sheet_marker = any(next_line_stripped_lower == f"{known_base_name}: table 1" for known_base_name in BASE_SHEET_NAMES_FOR_TABLE_1_PATTERN)
                if is_next_sheet_marker:
                    # print(f"  INFO: Stopping data for '{sheet_name_base}' at blank line {i+1}. Next line {i+2} is '{lines[i+1].strip()}'.")
                    break # End of current sheet's data
            else: # Blank line at the very end of the file
                # print(f"  INFO: Stopping data for '{sheet_name_base}' at blank line {i+1} (EOF).")
                break
            data_lines_for_parser.append(current_line_raw) # Include blank line if it's not an end-of-sheet marker
            continue

        current_line_lower = current_line_stripped.lower()
        is_new_sheet_marker = False
        # Check if the current line itself is a marker for a *different* sheet
        for known_base_name in BASE_SHEET_NAMES_FOR_TABLE_1_PATTERN:
            if current_line_lower == f"{known_base_name}: table 1":
                if known_base_name != sheet_name_base.lower(): # Make sure it's not the current sheet's marker again
                    # print(f"  INFO: Stopping data for '{sheet_name_base}'. Found different sheet marker '{current_line_stripped}' at line {i+1}.")
                    is_new_sheet_marker = True
                    break
        if is_new_sheet_marker:
            break # End of current sheet's data

        data_lines_for_parser.append(current_line_raw)

    # Special handling for 'vInfo': parse line-by-line for robustness and specific type conversions
    if sheet_name_base == 'vInfo':
        parsed_vms = []
        header_list_from_csv = []
        try:
            header_list_from_csv = [sanitize_column_name(h.strip()) for h in header_content_for_parsing.split(',')]
        except AttributeError: # header_content_for_parsing might not be a string if parsing failed unexpectedly
            print(f"  Error parsing header for {sheet_name_base}. Header content: {header_content_for_parsing}")
            return [] # Critical error for vInfo

        csv_reader = csv.reader(data_lines_for_parser)
        for row_idx, row_values in enumerate(csv_reader):
            if not any(field.strip() for field in row_values): continue # Skip truly empty rows
            # Heuristic: if a row has less than half the expected fields, it might be malformed
            if len(row_values) < len(header_list_from_csv) * 0.5 :
                print(f"  Skipping potentially malformed row in {sheet_name_base} (line {start_data_idx + row_idx + 1}): {row_values[:5]}...")
                continue

            vm_dict_raw = {} # Raw data from CSV row
            vm_dict_final = {} # Processed data with type conversions

            # Map CSV row values to sanitized header names
            for col_idx, col_name_csv in enumerate(header_list_from_csv):
                if col_idx < len(row_values):
                    vm_dict_raw[col_name_csv] = row_values[col_idx].strip()
                else: # Handle rows with fewer columns than header
                    vm_dict_raw[col_name_csv] = None

            # Populate final VM dictionary based on expected columns and apply specific conversions
            for expected_col_name in expected_cols: # Iterate through VINFO_COLS_ACTUAL
                sanitized_expected = sanitize_column_name(expected_col_name)
                raw_value = vm_dict_raw.get(sanitized_expected)

                if expected_col_name in ['CPUs', 'Memory', 'Provisioned MiB', 'In Use MiB', 'Disks', 'NICs', 'HW version']:
                    vm_dict_final[expected_col_name] = pd.to_numeric(str(raw_value).replace(',', ''), errors='coerce')
                    if pd.isna(vm_dict_final[expected_col_name]):
                        vm_dict_final[expected_col_name] = 0 # Default to 0 if conversion fails
                elif expected_col_name == 'Template':
                    vm_dict_final[expected_col_name] = str(raw_value).strip().upper() == 'TRUE' if raw_value is not None else False
                elif expected_col_name == 'Suspended To Memory':
                    vm_dict_final[expected_col_name] = str(raw_value).strip().upper() == 'TRUE' if raw_value is not None else False
                else:
                    vm_dict_final[expected_col_name] = raw_value

            if vm_dict_final.get('VM'): # Only add if VM name is present
                parsed_vms.append(vm_dict_final)
        print(f"  Successfully processed sheet '{sheet_name_base}' line-by-line, loaded {len(parsed_vms)} VM records.")
        return parsed_vms
    else:
        # For other sheets, use pandas for parsing the extracted text block
        if not data_lines_for_parser:
            print(f"  No data lines for pandas processing of '{sheet_name_base}'. Returning empty DataFrame.")
            return pd.DataFrame(columns=expected_cols)

        # Reconstruct the sheet as a single string for pandas to read
        full_sheet_text_for_pandas = header_content_for_parsing + "\n" + "".join(data_lines_for_parser)

        try:
            # low_memory=False can help with mixed types; skip_blank_lines=True handles empty lines within data
            df = pd.read_csv(StringIO(full_sheet_text_for_pandas), header=0, on_bad_lines='warn', dtype=str, low_memory=False, skip_blank_lines=True)
        except Exception as e:
            print(f"  Pandas read_csv error for sheet '{sheet_name_base}': {e}. Returning empty DataFrame.")
            return pd.DataFrame(columns=expected_cols)

        df.columns = [sanitize_column_name(col) for col in df.columns]

        # Define columns that should be numeric for specific sheets
        numeric_cols_map = {
            'vDisk': ['Capacity MiB'],
            'vSnapshot': ['Size MiB (vmsn)', 'Size MiB (total)'] # 'Size MiB (total)' might not always be present
        }

        if sheet_name_base in numeric_cols_map:
            for col_original_name in numeric_cols_map[sheet_name_base]:
                col_sanitized_name = sanitize_column_name(col_original_name)
                if col_sanitized_name in df.columns:
                    df[col_sanitized_name] = robust_to_numeric(df[col_sanitized_name])

        # Reconstruct DataFrame with exactly the expected columns, in the expected order.
        # This handles cases where CSV has more/less columns or in different order than expected.
        final_cols_data = {}
        missing_original_cols = []
        actual_df_cols_lower = {col.lower(): col for col in df.columns} # For case-insensitive matching

        for original_col_name_from_expected_list in expected_cols:
            sanitized_version_of_expected_col = sanitize_column_name(original_col_name_from_expected_list)
            # Try to find the column in the DataFrame, case-insensitively
            if sanitized_version_of_expected_col.lower() in actual_df_cols_lower:
                # Use the actual casing from the DataFrame for indexing
                final_cols_data[original_col_name_from_expected_list] = df[actual_df_cols_lower[sanitized_version_of_expected_col.lower()]]
            else:
                # If column is missing, add an empty Series of object type
                final_cols_data[original_col_name_from_expected_list] = pd.Series(dtype='object')
                missing_original_cols.append(original_col_name_from_expected_list)

        if missing_original_cols:
            print(f"Warning: For sheet '{sheet_name_base}', expected columns NOT FOUND (will be empty): {', '.join(missing_original_cols)}")

        df_final = pd.DataFrame(final_cols_data, columns=expected_cols) # Ensure correct column order
        print(f"  Successfully processed sheet '{sheet_name_base}' with pandas, loaded {len(df_final)} rows.")
        return df_final


def load_os_breakdown(file_path):
    """
    Loads the OS mapping CSV file which maps RVTools OS strings to standardized names
    and complexity profiles.
    Tries multiple common encodings if utf-8 fails.
    Returns a dictionary for easy lookup.
    """
    try:
        try:
            df = pd.read_csv(file_path, encoding='utf-8')
        except UnicodeDecodeError:
            try:
                df = pd.read_csv(file_path, encoding='latin1') # Common alternative
            except UnicodeDecodeError:
                df = pd.read_csv(file_path, encoding='cp1252') # Another Windows encoding
        # Standardize column names from the CSV (lower, underscore for spaces)
        df.columns = [str(col).strip().lower().replace(' ', '_') for col in df.columns]
        expected_csv_headers = ['os_reported_in_rvtools', 'os_mapped_name', 'os_complexity']

        if not all(col in df.columns for col in expected_csv_headers):
            print(f"Error: OS CSV missing columns. Expected: {expected_csv_headers}. Found: {df.columns.tolist()}")
            return {}

        # Create a dictionary mapping: os_string_from_rvtools -> {mapped_name, complexity}
        os_mapping = {
            str(r[expected_csv_headers[0]]).strip(): { # Key is the OS string from RVTools
                'mapped_name': str(r[expected_csv_headers[1]]).strip(),
                'complexity': str(r[expected_csv_headers[2]]).strip()
            }
            for _, r in df.iterrows() if str(r.get(expected_csv_headers[0],'')).strip() # Ensure key is not empty
        }
        return os_mapping
    except FileNotFoundError:
        print(f"Error: OS Breakdown file not found: {file_path}")
        return {}
    except Exception as e:
        print(f"Error loading OS Breakdown: {e}")
        return {}

def map_os_details(os_name_input, os_mapping):
    """
    Maps a raw OS string (from RVTools) to a standardized mapped_name and complexity_profile.
    Uses direct matches from os_mapping first, then tries synonym-based matching,
    and finally falls back to regex-based heuristics for common OS families.
    """
    if not isinstance(os_name_input, str) or not os_name_input.strip() or os_name_input.lower() == 'nan':
        return 'N/A', 'Undefined', 'N/A (Empty OS Name)' # Mapped Name, Complexity Profile, Original OS Name

    os_name_clean = os_name_input.strip()
    rvtools_os_lower = os_name_clean.lower()

    # 1. Direct match in the mapping file (case-sensitive for keys as loaded)
    if os_name_clean in os_mapping:
        details = os_mapping[os_name_clean]
        return details['mapped_name'], details['complexity'], os_name_clean

    # 2. Synonym and version-aware matching (more flexible)
    #    This tries to match a known distro from the mapping file (e.g., "RHEL 8")
    #    with variations in the RVTools string (e.g., "Red Hat Enterprise Linux 8.x").
    distro_synonyms = {
        "rhel": ["red hat enterprise linux", "rhel"], "centos": ["centos", "centos linux"],
        "ubuntu": ["ubuntu linux", "ubuntu"], "sles": ["suse linux enterprise server", "sles", "suse linux enterprise", "suse"],
        "oracle linux": ["oracle linux", "oracle enterprise linux"], "freebsd": ["freebsd"],
        "windows server": ["windows server", "microsoft windows server"], "windows": ["windows", "microsoft windows"]
    }
    for key_os_from_map, details_from_map in os_mapping.items():
        map_key_lower = key_os_from_map.lower().strip()
        # Try to parse "DistroName Version" from the mapping key
        map_key_match = re.match(r"([a-zA-Z\s.-]+?)\s*-?([\d\.]+[a-zA-Z\d]*)", map_key_lower)
        if map_key_match:
            map_distro_candidate = map_key_match.group(1).strip()
            map_version_candidate = map_key_match.group(2)
            matched_distro_in_rvtools = False
            for official_distro_key, synonym_list in distro_synonyms.items():
                if map_distro_candidate == official_distro_key or map_distro_candidate in synonym_list:
                    # Check if any synonym + version pattern matches the RVTools OS string
                    for syn in synonym_list:
                        pattern = rf"{re.escape(syn)}(\s*[-\(\s]*)?{re.escape(map_version_candidate)}"
                        if re.search(pattern, rvtools_os_lower):
                            matched_distro_in_rvtools = True
                            break
                    if matched_distro_in_rvtools:
                        break
            if matched_distro_in_rvtools:
                return details_from_map['mapped_name'], details_from_map['complexity'], os_name_clean

        # 3. Substring matching as a broader attempt (less precise)
        #    Avoids cross-matching Windows/Linux if one is clearly specified and the other is not.
        if map_key_lower in rvtools_os_lower or rvtools_os_lower in map_key_lower:
            if ("windows" in rvtools_os_lower and "windows" not in map_key_lower and "linux" in map_key_lower) or \
                    ("linux" in rvtools_os_lower and "linux" not in map_key_lower and "windows" in map_key_lower):
                continue # Skip if one is Linux and other is Windows to avoid bad matches
            return details_from_map['mapped_name'], details_from_map['complexity'], os_name_clean

    # 4. Fallback: Heuristic categorization based on keywords if no mapping found
    category, complexity_group = 'Unknown/Other', 'Unknown'
    if re.search(r'windows server|microsoft windows server', rvtools_os_lower): category, complexity_group = 'Windows Server Other', 'Windows Server'
    elif re.search(r'windows', rvtools_os_lower): category, complexity_group = 'Windows Desktop Other', 'Windows Desktop'
    elif re.search(r'rhel|red hat enterprise linux', rvtools_os_lower): category, complexity_group = 'RHEL Other', 'Linux'
    elif re.search(r'centos', rvtools_os_lower): category, complexity_group = 'CentOS Other', 'Linux'
    elif re.search(r'ubuntu', rvtools_os_lower): category, complexity_group = 'Ubuntu Other', 'Linux'
    elif re.search(r'suse|sles', rvtools_os_lower): category, complexity_group = 'SUSE Other', 'Linux'
    elif re.search(r'debian', rvtools_os_lower): category, complexity_group = 'Debian Other', 'Linux'
    elif re.search(r'oracle linux', rvtools_os_lower): category, complexity_group = 'Oracle Linux Other', 'Linux'
    elif re.search(r'freebsd', rvtools_os_lower): category, complexity_group = 'FreeBSD Other', 'BSD'
    elif re.search(r'linux', rvtools_os_lower): category, complexity_group = 'Linux Other', 'Linux' # Generic Linux
    elif re.search(r'esx|vmware', rvtools_os_lower): category, complexity_group = 'ESXi/Infrastructure', 'Infrastructure'
    elif re.search(r'solaris|sunos', rvtools_os_lower): category, complexity_group = 'Solaris', 'Legacy Unix'
    elif re.search(r'aix', rvtools_os_lower): category, complexity_group = 'AIX', 'Legacy Unix'
    elif re.search(r'hp-ux|hpux', rvtools_os_lower): category, complexity_group = 'HP-UX', 'Legacy Unix'

    return category, complexity_group, os_name_clean


def get_vmware_tools_status_category(tools_status_vinfo, tools_version_vinfo, tools_status_vtools_sheet, tools_version_vtools_sheet):
    """
    Determines a consolidated VMware Tools status category and reported version.
    It prioritizes information from the 'vTools' sheet if available,
    then falls back to 'vInfo' sheet data.
    """
    final_tools_status_str = ""
    final_tools_version_str = ""

    # Prioritize vTools sheet data for status and version if present and valid
    if tools_status_vtools_sheet and not pd.isna(tools_status_vtools_sheet) and str(tools_status_vtools_sheet).strip().lower() not in ['nan', 'none', 'unknown', 'not_installed', '']:
        final_tools_status_str = str(tools_status_vtools_sheet).lower()
    elif tools_status_vinfo and not pd.isna(tools_status_vinfo) and str(tools_status_vinfo).strip().lower() not in ['nan', 'none', '']: # Fallback to vInfo status
        final_tools_status_str = str(tools_status_vinfo).lower()

    if tools_version_vtools_sheet and not pd.isna(tools_version_vtools_sheet) and str(tools_version_vtools_sheet).strip().lower() not in ['nan', 'none', 'unknown', 'not_installed', '0', '']:
        final_tools_version_str = str(tools_version_vtools_sheet).strip()
    elif tools_version_vinfo and not pd.isna(tools_version_vinfo) and str(tools_version_vinfo).strip().lower() not in ['nan', 'none', '0', '']: # Fallback to vInfo version
        final_tools_version_str = str(tools_version_vinfo).strip()

    if not final_tools_status_str and not final_tools_version_str:
        return "Not Reported in CSV", "N/A" # Category, Version

    # Interpret status string
    if final_tools_status_str:
        if "not running" in final_tools_status_str or "toolsnotrunning" in final_tools_status_str:
            return "Not Running", final_tools_version_str or "N/A"
        if "not installed" in final_tools_status_str or "toolsnotinstalled" in final_tools_status_str or "vmware tools has never been installed" in final_tools_status_str:
            return "Not Installed", "N/A"
        # "toolsOk" (from vTools) or "current, running" / "ok, running" (from vInfo) implies good state
        if ("current" in final_tools_status_str and "running" in final_tools_status_str) or \
                ("ok" in final_tools_status_str and "running" in final_tools_status_str) or \
                ("toolsok" == final_tools_status_str):
            return "Running (OK / Current)", final_tools_version_str
        # "guesttoolsrunning" is a common status from vInfo
        if "guesttoolsrunning" in final_tools_status_str:
            if "versionisoutdated" in final_tools_status_str or "needsupgrade" in final_tools_status_str or "blacklisted" in final_tools_status_str:
                return "Running (Outdated/Issue)", final_tools_version_str
            return "Running (OK - guestToolsRunning)", final_tools_version_str # Default if just "guestToolsRunning"
        # Generic "running" status
        if "running" in final_tools_status_str:
            if "versionisoutdated" in final_tools_status_str or "needsupgrade" in final_tools_status_str or "blacklisted" in final_tools_status_str:
                return "Running (Outdated/Issue)", final_tools_version_str
            return "Running (Status Unclear)", final_tools_version_str or "N/A"
        if "unmanaged" in final_tools_status_str:
            return "Unmanaged", final_tools_version_str or "N/A"

    # If no status string but a version is reported, assume it's running
    if final_tools_version_str and final_tools_version_str not in ['0', '', 'N/A', 'not_installed', 'unknown']:
        return "Running (Version Reported Only)", final_tools_version_str

    return "Unknown Status", final_tools_version_str or "N/A"


def enrich_vm_data(vinfo_list_of_dicts, disks_df, nics_df, snapshots_df, os_mapping, vtools_df=None):
    """
    Enriches the core VM information (from vInfo) with data from other sheets
    (disks, NICs, snapshots, vTools) and computes migration-related considerations
    and a risk score.
    """
    enriched_vms = []
    if not vinfo_list_of_dicts:
        return enriched_vms

    # Convert list of dicts to DataFrame for easier merging with vTools
    vinfo_df_temp = pd.DataFrame(vinfo_list_of_dicts)

    # Merge vTools data if available, prioritizing it for Tools status/version
    if vtools_df is not None and not vtools_df.empty and 'VM' in vtools_df.columns:
        vtools_cols_to_use = ['VM']
        if 'Tools Version' in vtools_df.columns: vtools_cols_to_use.append('Tools Version')
        if 'Tools' in vtools_df.columns: vtools_cols_to_use.append('Tools') # This is the status column in vTools

        # Suffixes are used to distinguish columns if names clash (e.g. 'Tools Version' in both vInfo and vTools)
        vinfo_df_temp = pd.merge(vinfo_df_temp, vtools_df[vtools_cols_to_use], on='VM', how='left', suffixes=('', '_vtools_sheet_suffix'))

        # Rename merged vTools columns for clarity
        if 'Tools Version_vtools_sheet_suffix' in vinfo_df_temp.columns:
            vinfo_df_temp.rename(columns={'Tools Version_vtools_sheet_suffix': 'ToolsVersion_vtools_sheet'}, inplace=True)
        # Handle case where 'Tools Version' might only exist in vtools_df and not get a suffix if no clash
        elif 'Tools Version' in vtools_df.columns and 'Tools Version' not in pd.DataFrame(vinfo_list_of_dicts).columns and 'ToolsVersion_vtools_sheet' not in vinfo_df_temp.columns:
            if 'Tools Version_vtools_sheet_suffix' not in vinfo_df_temp.columns : # Defensive check
                vinfo_df_temp.rename(columns={'Tools Version': 'ToolsVersion_vtools_sheet'}, inplace=True)


        if 'Tools_vtools_sheet_suffix' in vinfo_df_temp.columns:
            vinfo_df_temp.rename(columns={'Tools_vtools_sheet_suffix': 'ToolsStatus_vtools_sheet'}, inplace=True)
        elif 'Tools' in vtools_df.columns and 'VMware Tools' not in pd.DataFrame(vinfo_list_of_dicts).columns and 'ToolsStatus_vtools_sheet' not in vinfo_df_temp.columns:
            if 'Tools_vtools_sheet_suffix' not in vinfo_df_temp.columns: # Defensive check
                vinfo_df_temp.rename(columns={'Tools': 'ToolsStatus_vtools_sheet'}, inplace=True)

    # Convert back to list of dicts for iterative processing
    vinfo_list_of_dicts = vinfo_df_temp.to_dict('records')

    for vm_detail_dict in vinfo_list_of_dicts:
        vm_name = vm_detail_dict.get('VM')
        if not vm_name: # Skip if VM name is missing
            continue

        # --- Initial Flags and OS Mapping ---
        is_excluded = any(pattern.fullmatch(vm_name) for pattern in EXCLUSION_REGEX_PATTERNS)
        vm_detail_dict['is_excluded_vm'] = is_excluded
        vm_detail_dict['is_template'] = vm_detail_dict.get('Template', False)

        # Determine primary OS string for mapping (prefer Tools, fallback to config)
        os_from_tools = vm_detail_dict.get('OS according to the VMware Tools', '')
        os_primary_for_mapping = os_from_tools
        if pd.isna(os_from_tools) or not str(os_from_tools).strip(): # If OS from Tools is missing/empty
            os_config_file = vm_detail_dict.get('OS according to the configuration file', 'N/A')
            os_primary_for_mapping = os_config_file
        if pd.isna(os_primary_for_mapping): os_primary_for_mapping = 'N/A' # Ensure it's a string
        mapped_name, os_complexity_profile, original_os_name_used = map_os_details(str(os_primary_for_mapping), os_mapping)
        vm_detail_dict.update({'OS_Primary': original_os_name_used, 'OS_Mapped_Name': mapped_name, 'OS_Profile': os_complexity_profile})

        # --- Disk Information ---
        current_vm_disks_df = disks_df[disks_df['VM'] == vm_name] if not disks_df.empty and 'VM' in disks_df.columns else pd.DataFrame()
        vm_detail_dict['num_disks'] = len(current_vm_disks_df) if not current_vm_disks_df.empty else 0

        if not current_vm_disks_df.empty and 'Capacity MiB' in current_vm_disks_df.columns:
            capacities = robust_to_numeric(current_vm_disks_df['Capacity MiB'], default=0.0)
            vm_detail_dict['TotalDiskCapacityMiB'] = capacities.sum()
            vm_detail_dict['max_disk_size_mib'] = capacities.max() if not capacities.empty else 0.0
        else:
            vm_detail_dict['TotalDiskCapacityMiB'] = 0.0
            vm_detail_dict['max_disk_size_mib'] = 0.0

        disk_detail_cols_to_extract = ['Disk Path', 'Capacity MiB', 'Thin', 'Controller']
        vm_detail_dict['DiskDetails'] = current_vm_disks_df[[col for col in disk_detail_cols_to_extract if col in current_vm_disks_df.columns]].to_dict('records') if not current_vm_disks_df.empty else []

        # --- NIC Information ---
        current_vm_nics_df = nics_df[nics_df['VM'] == vm_name] if not nics_df.empty and 'VM' in nics_df.columns else pd.DataFrame()
        nic_detail_cols_to_extract = ['NIC label', 'Network', 'Adapter', 'IPv4 Address', 'Connected']
        vm_detail_dict['NICDetails'] = current_vm_nics_df[[col for col in nic_detail_cols_to_extract if col in current_vm_nics_df.columns]].to_dict('records') if not current_vm_nics_df.empty else []
        # Get total NICs from vInfo (often more reliable for count than summing from vNetwork which might miss some)
        vm_detail_dict['TotalNICs'] = robust_to_numeric(pd.Series([vm_detail_dict.get('NICs')]), default=0).iloc[0]


        # --- Migration Considerations and Risk Scoring ---
        mc = {} # Dictionary to hold all migration consideration details
        mc['risk_score'] = 0
        mc['risk_score_reasons'] = [] # List to store reasons for score increments

        # CBT (Changed Block Tracking) Check based on 'Change Version'
        change_version = vm_detail_dict.get('Change Version', '')
        if change_version and str(change_version).strip() and str(change_version).lower() != 'nan':
            mc['cbt_status_notes'] = f"CBT likely enabled (Change Version: {change_version}). Warm migration may be possible."
        else:
            mc['cbt_status_notes'] = "CBT status unknown or disabled (Change Version field empty/NA). Manual verification recommended for warm migrations."

        # Hibernation/Suspend Check
        suspended_to_memory = vm_detail_dict.get('Suspended To Memory', False) # Boolean from vInfo processing
        suspend_time = vm_detail_dict.get('Suspend time', '')
        if suspended_to_memory or (suspend_time and str(suspend_time).strip() and str(suspend_time).lower() != 'nan'):
            mc['hibernation_notes'] = f"VM may be suspended or have hibernation enabled (SuspendedToMemory: {suspended_to_memory}, SuspendTime: {suspend_time}). Verify hibernation is disabled on source VM."
        else:
            mc['hibernation_notes'] = "Hibernation appears disabled or VM not suspended."

        # Handle excluded VMs and templates: they don't get a risk score
        if is_excluded or vm_detail_dict['is_template']:
            mc['risk_score'] = "N/A" # Not applicable
            if vm_detail_dict['is_template']:
                mc['risk_score_reasons'].append("VM is a Template - Excluded from scoring.")
                mc['excluded_reason'] = "VM is a Template."
                mc['migration_recommendation_category'] = "EXCLUDE - TEMPLATE"
            else: # is_excluded (infrastructure VM)
                mc['risk_score_reasons'].append("Infrastructure VM - Excluded from scoring.")
                mc['excluded_reason'] = "Infrastructure VM based on naming convention."
                mc['migration_recommendation_category'] = "EXCLUDE - INFRASTRUCTURE VM"
            vm_detail_dict['migration_considerations'] = mc
            enriched_vms.append(vm_detail_dict)
            continue # Move to the next VM

        # Factor 1: OS Profile
        mc['os_profile_from_mapping'] = vm_detail_dict.get('OS_Profile', 'Unknown')
        os_profile_points = 0
        # Higher points for OS profiles indicating significant rework or incompatibility
        if mc['os_profile_from_mapping'] in ['Hard', 'Recreate', 'Retire', 'Legacy Unix', 'Unknown/Other', 'Unknown', 'nan', 'Undefined', 'N/A']:
            os_profile_points = 3
        elif mc['os_profile_from_mapping'] == 'Medium':
            os_profile_points = 1
        if os_profile_points > 0:
            mc['risk_score_reasons'].append(f"OS Profile '{mc['os_profile_from_mapping']}': +{os_profile_points}")
            mc['risk_score'] += os_profile_points

        # Factor 2: VMware Tools Status
        # Consolidate tools info from vInfo and potentially vTools sheet (done during merge)
        tools_status_vinfo = vm_detail_dict.get('VMware Tools')       # From vInfo sheet
        tools_version_vinfo = vm_detail_dict.get('VMware Tools Version') # From vInfo sheet
        tools_status_vtools_sheet = vm_detail_dict.get('ToolsStatus_vtools_sheet') # From vTools sheet (if merged)
        tools_version_vtools_sheet = vm_detail_dict.get('ToolsVersion_vtools_sheet') # From vTools sheet (if merged)
        mc['tools_status_category'], mc['tools_status_version_reported'] = get_vmware_tools_status_category(
            tools_status_vinfo, tools_version_vinfo, tools_status_vtools_sheet, tools_version_vtools_sheet
        )
        tools_points = 0
        if mc['tools_status_category'] in ["Not Installed", "Not Running", "Unknown Status", "Not Reported in CSV"]:
            tools_points = 3
        elif mc['tools_status_category'] == "Running (Outdated/Issue)":
            tools_points = 2
        elif mc['tools_status_category'] == "Running (Status Unclear)":
            tools_points = 1
        if tools_points > 0:
            mc['risk_score_reasons'].append(f"Tools '{mc['tools_status_category']}': +{tools_points}")
            mc['risk_score'] += tools_points

        # Factor 3: Hardware Version
        # HW versions are typically like 'vmx-15'. We need the number.
        hw_version_str = str(vm_detail_dict.get('HW version', '0')).split('.')[0] # Take integer part if "vmx-11.0"
        hw_version_str = re.sub(r'\D', '', hw_version_str) # Remove non-digits like 'vmx-'
        hw_version = 0
        if hw_version_str.isdigit():
            hw_version = int(hw_version_str)

        mc['hw_version_notes'] = "N/A"
        hw_points = 0
        if hw_version < 11 and hw_version != 0 : # HW version < 11 is pre-vSphere 6.0 equivalent
            mc['hw_version_notes'] = f"Older HW version ({hw_version}), consider upgrade pre-migration."
            hw_points = 1
        elif hw_version == 0: # Treat 0 as Unknown
            mc['hw_version_notes'] = "Unknown HW Version."
            hw_points = 1
        if hw_points > 0:
            mc['risk_score_reasons'].append(f"Hardware Version ({hw_version if hw_version !=0 else 'Unknown'}): +{hw_points}")
            mc['risk_score'] += hw_points

        # Factor 4: Disk Controller Compatibility (for MTV)
        vm_controllers = [str(d.get('Controller', '')).lower().strip() for d in vm_detail_dict.get('DiskDetails', []) if d.get('Controller')]
        vm_controllers_present_str = ", ".join(sorted(list(set(c for c in vm_controllers if c)))) or "None Reported"
        # Check for presence of known compatible controllers
        is_compatible_controller_present = any(any(compat_ctrl.lower() in c for compat_ctrl in MTV_COMPATIBLE_DISK_CONTROLLERS) for c in vm_controllers)
        # Specifically check for NVMe, which is problematic for MTV
        is_nvme_present = any(any(unsupp_ctrl.lower() in c for unsupp_ctrl in MTV_UNSUPPORTED_DISK_CONTROLLERS) for c in vm_controllers)

        disk_controller_points = 0
        if is_nvme_present:
            mc['disk_controller_notes'] = f"NVMe controller present. MTV migration not directly supported. (Present: {vm_controllers_present_str})"
            disk_controller_points = 5 # High penalty for NVMe with MTV
        elif not is_compatible_controller_present and vm_controllers: # Only penalize if disks exist and none are compatible
            mc['disk_controller_notes'] = f"No MTV-compatible disk controller found. (Present: {vm_controllers_present_str})"
            disk_controller_points = 1
        else: # Either compatible found or no controllers reported (e.g., ISO-only VM)
            mc['disk_controller_notes'] = f"Compatible disk controller(s) detected or no disk controllers reported. (Present: {vm_controllers_present_str})"

        if disk_controller_points > 0:
            mc['risk_score_reasons'].append(f"Disk Controllers: +{disk_controller_points}")
            mc['risk_score'] += disk_controller_points

        # Factor 5: Network Adapter Compatibility (for MTV)
        vm_nic_adapters = [str(n.get('Adapter', '')).lower().strip() for n in vm_detail_dict.get('NICDetails', []) if n.get('Adapter')]
        vm_nic_types_present_str = ", ".join(sorted(list(set(na for na in vm_nic_adapters if na and na != 'false')))) or "None Reported" # Filter out 'false' if it appears

        non_preferred_nics = []
        all_preferred = True
        if not vm_nic_adapters or not vm_nic_types_present_str : # No NICs reported, or all were 'false' or empty
            all_preferred = False # Treat as an issue if no valid NICs are identifiable
            if not vm_nic_types_present_str and vm_detail_dict.get('TotalNICs', 0) > 0: # NICs reported in vInfo but not in vNetwork detail
                non_preferred_nics.append("Type Unknown/Not in vNetwork")
        else:
            for nic_adapter in vm_nic_adapters:
                if nic_adapter and nic_adapter != 'false': # Process actual adapter types
                    is_preferred = any(preferred.lower() in nic_adapter for preferred in MTV_PREFERRED_NETWORK_ADAPTERS)
                    if not is_preferred:
                        all_preferred = False
                        non_preferred_nics.append(nic_adapter)

        nic_points = 0
        if not all_preferred:
            reason_str = f"{', '.join(sorted(list(set(non_preferred_nics))))}" if non_preferred_nics else "Type Unknown/Not Reported"
            mc['network_adapter_notes'] = f"Non-preferred NIC(s) found: {reason_str}. Recommended: {', '.join(MTV_PREFERRED_NETWORK_ADAPTERS)}. (Types Present: {vm_nic_types_present_str})"
            nic_points = 1
        else:
            mc['network_adapter_notes'] = f"All network adapter types are preferred or no NICs reported. (Types Present: {vm_nic_types_present_str})"

        if nic_points > 0:
            mc['risk_score_reasons'].append(f"Network Adapters: +{nic_points}")
            mc['risk_score'] += nic_points

        # Factor 6: Snapshots
        current_vm_snapshots_df = snapshots_df[snapshots_df['VM'] == vm_name] if not snapshots_df.empty and 'VM' in snapshots_df.columns else pd.DataFrame()
        num_snapshots = len(current_vm_snapshots_df)
        snapshot_points = 0
        if num_snapshots > 0:
            mc['snapshots_info'] = f"{num_snapshots} found (Exist: Yes) - Notes: Must be consolidated before migration."
            mc['SnapshotDetails'] = current_vm_snapshots_df[['Name', 'Date / time']].to_dict('records') # Store snapshot names and dates
            snapshot_points = 3 # Snapshots are a significant migration blocker
        else:
            mc['snapshots_info'] = "0 found (Exist: No) - Notes: N/A"
            mc['SnapshotDetails'] = []
        if snapshot_points > 0:
            mc['risk_score_reasons'].append(f"Snapshots Present: +{snapshot_points}")
            mc['risk_score'] += snapshot_points

        # Factor 7: Firmware (BIOS vs EFI)
        firmware = str(vm_detail_dict.get('Firmware', 'Unknown')).lower() # From vInfo
        os_mapped = vm_detail_dict.get('OS_Mapped_Name', '').lower() # Use the mapped OS name for this check
        firmware_points = 0
        if firmware == 'unknown':
            mc['firmware_notes'] = "Firmware type is Unknown."
            firmware_points = 1
        # Modern Windows OSes strongly prefer/require EFI
        elif any(win_os in os_mapped for win_os in ['windows 10', 'windows 11', 'windows server 2019', 'windows server 2022', 'windows server 2016']) and firmware != 'efi':
            mc['firmware_notes'] = f"Firmware is {firmware.capitalize()}. EFI is strongly recommended for modern Windows OS ({os_mapped})."
            firmware_points = 2
        else:
            mc['firmware_notes'] = f"Firmware is {firmware.capitalize()}. {'EFI firmware reported.' if firmware == 'efi' else 'BIOS firmware reported; EFI is generally preferred for newer OS.'}"

        if firmware_points > 0:
            mc['risk_score_reasons'].append(f"Firmware ({firmware.capitalize()}): +{firmware_points}")
            mc['risk_score'] += firmware_points

        # Factor 8: Passthrough Devices
        # 'Fixed Passthru HotPlug' seems to be a boolean field indicating presence
        passthru_str = str(vm_detail_dict.get('Fixed Passthru HotPlug', 'false')).lower()
        passthru_points = 0
        if passthru_str == 'true':
            mc['passthrough_notes'] = "Passthrough devices indicated. Migration requires special handling or device removal."
            passthru_points = 5 # Passthrough is a major migration complexity
        else:
            mc['passthrough_notes'] = "No passthrough devices indicated."
        if passthru_points > 0:
            mc['risk_score_reasons'].append(f"Passthrough Devices: +{passthru_points}")
            mc['risk_score'] += passthru_points

        # --- Migration Recommendation Category (Simplified) ---
        # Based on the total risk score and critical OS profiles
        if mc['risk_score'] >= 5 or mc['os_profile_from_mapping'] in ['Hard', 'Recreate', 'Retire', 'Legacy Unix', 'Unknown/Other', 'Unknown', 'nan', 'Undefined', 'N/A']:
            mc['migration_recommendation_category'] = "REVIEW / REHOST / REPLATFORM / REFACTOR"
        elif mc['risk_score'] >=3:
            mc['migration_recommendation_category'] = "MEDIUM COMPLEXITY"
        else:
            mc['migration_recommendation_category'] = "LOW COMPLEXITY / EASIER MIGRATION"

        vm_detail_dict['migration_considerations'] = mc
        enriched_vms.append(vm_detail_dict)
    return enriched_vms


def generate_report_text(enriched_vms, rvtools_fpath, os_map_fpath):
    """
    Generates a formatted text report from the enriched VM data.
    Includes summaries, risk score explanations, and detailed per-VM analysis.
    """
    report_lines = []

    # Attempt to get VI SDK API Version from the first VM in the list.
    # This assumes a single vCenter context for the RVTools export.
    first_vm_sdk_version = "Unknown"
    if enriched_vms:
        # Check if the key exists and is not NaN or empty
        sdk_version_raw = enriched_vms[0].get('VI SDK API Version')
        if sdk_version_raw and not pd.isna(sdk_version_raw) and str(sdk_version_raw).strip():
            first_vm_sdk_version = str(sdk_version_raw).strip()
        else:
            first_vm_sdk_version = "Not explicitly found in vInfo for first VM"

    report_lines.append("VM MIGRATION ANALYSIS REPORT")
    report_lines.append(f"Source RVTools File: {os.path.basename(rvtools_fpath)}")
    report_lines.append(f"OS Breakdown File: {os.path.basename(os_map_fpath)}")
    report_lines.append(f"VI SDK API Version (from first VM): {first_vm_sdk_version}")
    report_lines.append(f"Generated on: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 80)
    report_lines.append("IMPORTANT NOTES & MANUAL CHECKS REQUIRED (General for MTV/OpenShift Virtualization):")
    report_lines.append("  - Ensure login as a user with the minimal set of VMware privileges for migration tasks.")
    report_lines.append("  - Verify a VMware Virtual Disk Development Kit (VDDK) image is created and accessible to OpenShift Virtualization if using warm migration.")
    report_lines.append("  - If migrating >10 VMs from a single ESXi host in one migration plan, check/increase NFC service memory on the ESXi host.")
    report_lines.append("  - This script provides a heuristic risk score. Always cross-reference with detailed VM data, application owner input, and project requirements.")
    report_lines.append("  - Review specific MTV documentation for supported OS versions, hardware compatibility, and limitations.")
    report_lines.append("=" * 80)

    # Separate VMs into categories for reporting
    templates = [vm for vm in enriched_vms if vm.get('is_template', False)]
    # Migration candidates are not excluded AND not templates
    migration_candidates = [vm for vm in enriched_vms if not vm.get('is_excluded_vm', False) and not vm.get('is_template', False)]
    # Excluded VMs are those matching exclusion patterns AND are not templates (templates handled separately)
    excluded_by_pattern_vms = [vm for vm in enriched_vms if vm.get('is_excluded_vm', True) and not vm.get('is_template', False)]

    report_lines.append(f"TOTAL VMS IN RVTOOLS FILE (after initial parsing): {len(enriched_vms)}")
    report_lines.append(f"  - Detected Templates (Excluded from Detailed Migration Analysis): {len(templates)}")
    report_lines.append(f"  - VMs Excluded by Name Pattern (Infrastructure/Special, Excluded from Detailed Migration Analysis): {len(excluded_by_pattern_vms)}")
    report_lines.append(f"  - VMs Considered for Detailed Migration Analysis (Migration Candidates): {len(migration_candidates)}")
    report_lines.append("=" * 80)
    report_lines.append("\nOVERALL SUMMARY (Migration Candidates)")
    report_lines.append("-" * 80)

    if migration_candidates:
        powered_on_vms = sum(1 for vm in migration_candidates if str(vm.get('Powerstate', '')).lower() == 'poweredon')
        # Ensure numeric types for summation, defaulting to 0 if None/NaN
        total_cpus = sum(vm.get('CPUs', 0) or 0 for vm in migration_candidates)
        total_memory_mib = sum(vm.get('Memory', 0) or 0 for vm in migration_candidates)
        total_prov_storage_mib_vinfo = sum(vm.get('Provisioned MiB', 0) or 0 for vm in migration_candidates)
        total_inuse_storage_mib_vinfo = sum(vm.get('In Use MiB', 0) or 0 for vm in migration_candidates)
        total_disk_cap_mib_vdisk = sum(vm.get('TotalDiskCapacityMiB', 0.0) or 0.0 for vm in migration_candidates)

        report_lines.append(f"  - Powered On VMs: {powered_on_vms}")
        report_lines.append(f"  - Powered Off VMs: {len(migration_candidates) - powered_on_vms}")
        report_lines.append(f"  - Total vCPUs Allocated: {total_cpus}")
        report_lines.append(f"  - Total Memory Allocated: {total_memory_mib / 1024:.2f} GiB (approx. {total_memory_mib} MiB)")
        report_lines.append(f"  - Total Storage Provisioned (from vInfo 'Provisioned MiB'): {total_prov_storage_mib_vinfo / (1024*1024):.2f} TiB (approx. {total_prov_storage_mib_vinfo / 1024:.2f} GiB)")
        report_lines.append(f"  - Total Storage In Use (from vInfo 'In Use MiB'): {total_inuse_storage_mib_vinfo / (1024*1024):.2f} TiB (approx. {total_inuse_storage_mib_vinfo / 1024:.2f} GiB)")
        report_lines.append(f"  - Total Disk Capacity (sum from vDisk 'Capacity MiB'): {total_disk_cap_mib_vdisk / (1024*1024):.2f} TiB (approx. {total_disk_cap_mib_vdisk / 1024:.2f} GiB)")
    else:
        report_lines.append("  No migration candidates to summarize.")
    report_lines.append("\n" + "=" * 80)
    report_lines.append("CALCULATED RISK SCORE EXPLANATION & STATISTICS (For Migration Candidates)")
    report_lines.append("=" * 80)
    report_lines.append("The 'Calculated Risk Score' is a heuristic value to help quantify potential migration complexities.")
    report_lines.append("Higher scores indicate more potential issues that may require manual intervention or careful planning.")
    report_lines.append("It is derived by summing points from the following factors:")
    report_lines.append("  - OS Profile (from your OS mapping CSV):")
    report_lines.append("    - +3 points: If OS Profile is 'Hard', 'Recreate', 'Retire', 'Legacy Unix', 'Unknown/Other', 'Unknown', 'nan', 'Undefined', or 'N/A'.")
    report_lines.append("    - +1 point: If OS Profile is 'Medium'.")
    report_lines.append("  - VMware Tools Status (Interpreted from vInfo and vTools sheets):")
    report_lines.append("    - +3 points: If 'Not Installed', 'Not Running', 'Unknown Status', or 'Not Reported in CSV'.")
    report_lines.append("    - +2 points: If 'Running (Outdated/Issue)'.")
    report_lines.append("    - +1 point: If 'Running (Status Unclear)'.")
    report_lines.append("  - Virtual Hardware Version (from vInfo):")
    report_lines.append("    - +1 point: If HW version < 11 (pre-vSphere 6.0 equivalent) or 'Unknown'.")
    report_lines.append("  - Disk Controllers (from vDisk - General MTV/KVM Compatibility):")
    report_lines.append(f"    - +5 points: If any controller is an unsupported type like {', '.join(MTV_UNSUPPORTED_DISK_CONTROLLERS)} (direct MTV migration likely not supported).")
    report_lines.append(f"    - +1 point: If NO detected disk controller is in the generally compatible list ({', '.join(MTV_COMPATIBLE_DISK_CONTROLLERS)}) and controllers are present.")
    report_lines.append("  - Network Adapters (from vNetwork - General MTV/KVM Compatibility):")
    report_lines.append(f"    - +1 point: If any adapter is not in the preferred list ({', '.join(MTV_PREFERRED_NETWORK_ADAPTERS)}), or if adapter type is Unknown/Not Reported for an existing NIC.")
    report_lines.append("  - Snapshots (from vSnapshot sheet):")
    report_lines.append("    - +3 points: If any snapshots are present (must be consolidated pre-migration).")
    report_lines.append("  - Firmware (from vInfo):")
    report_lines.append("    - +1 point: If firmware type is 'Unknown'.")
    report_lines.append("    - +2 points: If OS appears to be modern Windows (e.g., Server 2016+, Win 10+) and firmware is not 'Efi'.")
    report_lines.append("  - Passthrough Devices (from vInfo 'Fixed Passthru HotPlug'):")
    report_lines.append("    - +5 points: If passthrough devices are indicated ('true') (requires removal or special handling).")

    if migration_candidates:
        # Count VMs per risk score, excluding "N/A" scores (which are for excluded/template VMs already filtered out)
        risk_scores = Counter(vm['migration_considerations']['risk_score'] for vm in migration_candidates)
        report_lines.append("\n  VM Count by Calculated Risk Score (Migration Candidates):")
        if risk_scores:
            for score, count in sorted(risk_scores.items()):
                report_lines.append(f"    - Risk Score {score}: {count} VM(s)")
        else:
            report_lines.append("    No migration candidates with valid scores.")
    else:
        report_lines.append("\n  No migration candidates to score.")
    report_lines.append("=" * 80)

    # --- DETECTED TEMPLATES SECTION ---
    if templates:
        report_lines.append("\n\n================================================================================")
        report_lines.append("DETECTED TEMPLATES (Excluded from Detailed Migration Analysis)")
        report_lines.append("================================================================================")
        for vm in sorted(templates, key=lambda x: x.get('VM', '')): # Sort by VM name
            report_lines.append(f"\nVM Template: {vm.get('VM', 'N/A')}")
            report_lines.append(f"  Power State: {vm.get('Powerstate', 'N/A')}")
            report_lines.append(f"  RVTools OS (Primary Source for Mapping): {vm.get('OS_Primary', 'N/A')}")
            report_lines.append(f"  Mapped OS Name: {vm.get('OS_Mapped_Name', 'N/A')}")
            report_lines.append(f"  OS Profile (from Mapping): {vm.get('OS_Profile', 'N/A')}")
            report_lines.append(f"  CPUs: {vm.get('CPUs', 'N/A')}, Memory: {vm.get('Memory', 'N/A')} MiB")
            report_lines.append(f"  Provisioned Storage (vInfo 'Provisioned MiB'): {vm.get('Provisioned MiB', 'N/A')} MiB")
            report_lines.append(f"  Total Disk Capacity (vDisk sum 'Capacity MiB'): {vm.get('TotalDiskCapacityMiB', 'N/A')} MiB")
            report_lines.append(f"  Annotation: {str(vm.get('Annotation', '') or 'N/A').strip()}") # Display N/A if annotation is empty or None
            report_lines.append("-" * 60)
        report_lines.append("=" * 80)

    # --- EXCLUDED VMS BY NAME PATTERN SECTION ---
    if excluded_by_pattern_vms:
        report_lines.append("\n\n================================================================================")
        report_lines.append("VMs EXCLUDED BY NAME PATTERN (Infrastructure/Special, Excluded from Detailed Migration Analysis)")
        report_lines.append("================================================================================")
        for vm in sorted(excluded_by_pattern_vms, key=lambda x: x.get('VM', '')): # Sort by VM name
            mc = vm.get('migration_considerations', {})
            report_lines.append(f"\nExcluded VM: {vm.get('VM', 'N/A')}")
            report_lines.append(f"  Exclusion Reason: {mc.get('excluded_reason', 'N/A')}")
            report_lines.append(f"  Power State: {vm.get('Powerstate', 'N/A')}")
            report_lines.append(f"  RVTools OS (Primary Source for Mapping): {vm.get('OS_Primary', 'N/A')}")
            report_lines.append(f"  Mapped OS Name: {vm.get('OS_Mapped_Name', 'N/A')}")
            report_lines.append(f"  OS Profile (from Mapping): {vm.get('OS_Profile', 'N/A')}")
            report_lines.append(f"  CPUs: {vm.get('CPUs', 'N/A')}, Memory: {vm.get('Memory', 'N/A')} MiB")
            report_lines.append(f"  Provisioned Storage (vInfo 'Provisioned MiB'): {vm.get('Provisioned MiB', 'N/A')} MiB")
            report_lines.append(f"  Total Disk Capacity (vDisk sum 'Capacity MiB'): {vm.get('TotalDiskCapacityMiB', 'N/A')} MiB")
            report_lines.append(f"  Annotation: {str(vm.get('Annotation', '') or 'N/A').strip()}")
            report_lines.append("-" * 60)
        report_lines.append("=" * 80)


    # --- DETAILED VM INFORMATION SECTION (Migration Candidates) ---
    report_lines.append("\n\n================================================================================")
    report_lines.append("DETAILED VM INFORMATION & MIGRATION CONSIDERATIONS (Migration Candidates)")
    report_lines.append("================================================================================")

    # Group VMs by the determined migration recommendation category for structured reporting
    grouped_vms = defaultdict(list)
    for vm in migration_candidates:
        category = vm.get('migration_considerations', {}).get('migration_recommendation_category', "Unknown Category")
        grouped_vms[category].append(vm)

    # Define a sort order for the categories to make the report more readable
    sorted_categories = sorted(grouped_vms.keys(), key=lambda k: (
        0 if "LOW COMPLEXITY" in k else
        1 if "MEDIUM COMPLEXITY" in k else
        2 if "REVIEW" in k else # Covers "REVIEW / REHOST / REPLATFORM / REFACTOR"
        3 # Fallback for any other unforeseen categories
    ))

    for category in sorted_categories:
        vms_in_category = sorted(grouped_vms[category], key=lambda x: x.get('VM', '')) # Sort VMs by name within each category
        report_lines.append(f"\n\nGrouping by :\n=== MIGRATION RECOMMENDATION CATEGORY: {category.upper()} ===")
        report_lines.append(f"    ({len(vms_in_category)} VM(s) in this category)")

        for vm in vms_in_category:
            mc = vm.get('migration_considerations', {}) # Migration considerations dictionary for this VM
            report_lines.append(f"\nVM: {vm.get('VM', 'N/A')}")

            # Summary of key migration indicators
            risk_score = mc.get('risk_score', 'N/A')
            risk_reasons_str = ""
            if mc.get('risk_score_reasons'):
                risk_reasons_str = f" (Reasons: {'; '.join(mc['risk_score_reasons'])})"
            report_lines.append(f"  (I)   Calculated Risk Score  : {risk_score}{risk_reasons_str}")
            report_lines.append(f"  (II)  OS & Complexity        : Mapped OS - '{vm.get('OS_Mapped_Name', 'N/A')}', Profile - '{vm.get('OS_Profile', 'N/A')}' (From: '{vm.get('OS_Primary', 'N/A')}')")

            max_disk_gib_approx = (vm.get('max_disk_size_mib', 0.0) or 0.0) / 1024
            max_disk_str = f"{max_disk_gib_approx:.1f} GiB" if max_disk_gib_approx >= 1 else f"{vm.get('max_disk_size_mib', 0.0):.0f} MiB"
            report_lines.append(f"  (III) Storage Summary        : {vm.get('num_disks', 'N/A')} disk(s), Max Disk Size (vDisk) - {max_disk_str}")
            report_lines.append(f"  (IV)  Functional Purpose (Ann.): {str(vm.get('Annotation', '') or 'N/A').strip()}") # Display N/A if annotation is empty/None
            report_lines.append("  ---- Detailed Attributes & Considerations ----")
            report_lines.append(f"  Power State               : {vm.get('Powerstate', 'N/A')}")
            report_lines.append(f"  CPUs                      : {vm.get('CPUs', 'N/A')}")
            report_lines.append(f"  Memory (MiB)              : {vm.get('Memory', 'N/A')}")
            report_lines.append(f"  Provisioned Storage (MiB) : {vm.get('Provisioned MiB', 'N/A')} (vInfo)")
            report_lines.append(f"  In Use Storage (MiB)      : {vm.get('In Use MiB', 'N/A')} (vInfo)")
            report_lines.append(f"  Total Disk Capacity (MiB) : {vm.get('TotalDiskCapacityMiB', 'N/A')} (vDisk sum)")
            report_lines.append(f"  Number of Disks (vDisk)   : {vm.get('num_disks', 'N/A')}")
            report_lines.append(f"  Max Disk Size (MiB vDisk) : {vm.get('max_disk_size_mib', 'N/A')}")
            report_lines.append(f"  VMware Tools Status       : {mc.get('tools_status_category', 'N/A')}")
            report_lines.append(f"  VMware Tools Version      : {mc.get('tools_status_version_reported', 'N/A')}")
            report_lines.append(f"  Hardware Version          : {vm.get('HW version', 'N/A')}")
            report_lines.append(f"  Cluster                   : {vm.get('Cluster', 'N/A')}")
            report_lines.append(f"  Resource Pool             : {vm.get('Resource pool', 'N/A')}")
            report_lines.append(f"  Folder Path               : {str(vm.get('Folder', '') or 'N/A').strip()}")
            report_lines.append(f"  Number of NICs (vInfo)    : {vm.get('TotalNICs', 'N/A')}") # Using TotalNICs from enriched data

            report_lines.append("  Migration Considerations (Analysis):")
            # Primary risk factors are those that contributed points to the score
            primary_risk_factors = [item.split(':')[0].strip() for item in mc.get('risk_score_reasons', []) if not isinstance(item, str) or int(item.split(': +')[1]) > 0] if mc.get('risk_score_reasons') else []
            report_lines.append(f"    - Primary Risk Factors Identified: {'; '.join(primary_risk_factors) if primary_risk_factors else 'None (Low Risk Score)'}")
            report_lines.append(f"    - Tools Status Notes: {mc.get('tools_status_category', 'N/A')}. Version: {mc.get('tools_status_version_reported', 'N/A')}. Consider update if not current/ok.")
            report_lines.append(f"    - Hardware Version Notes: {mc.get('hw_version_notes', 'N/A')}")
            report_lines.append(f"    - Firmware: {str(vm.get('Firmware', 'N/A')).capitalize()} - Notes: {mc.get('firmware_notes', 'N/A')}")
            report_lines.append(f"    - Disk Controller Notes: {mc.get('disk_controller_notes', 'N/A')}")
            report_lines.append(f"    - Network Adapter Notes: {mc.get('network_adapter_notes', 'N/A')}")
            report_lines.append(f"    - Snapshots Info: {mc.get('snapshots_info', 'N/A')}")
            if mc.get('SnapshotDetails'): # List snapshot details if they exist
                for snap_idx, snap_detail in enumerate(mc['SnapshotDetails']):
                    report_lines.append(f"      - Snapshot {snap_idx+1}: Name: '{snap_detail.get('Name', 'N/A')}', Created: {snap_detail.get('Date / time', 'N/A')}")
            report_lines.append(f"    - Passthrough Devices (HotPlug field): {'Yes' if str(vm.get('Fixed Passthru HotPlug','false')).lower() == 'true' else 'No'} - Notes: {mc.get('passthrough_notes', 'N/A')}")
            report_lines.append(f"    - CBT Status (Change Version from vInfo): '{vm.get('Change Version', 'N/A')}' - Notes: {mc.get('cbt_status_notes', 'N/A')}")
            report_lines.append(f"    - Hibernation/Suspend Status Notes: {mc.get('hibernation_notes', 'N/A')}")
            report_lines.append("-" * 60) # Separator for each VM's detailed entry

    return "\n".join(report_lines)

def generate_csv_report(enriched_vms, output_csv_fpath):
    """
    Generates a CSV report from the enriched VM data, with one VM per row.
    """
    print(f"\nGenerating CSV report: {output_csv_fpath}")

    csv_data = []
    for vm in enriched_vms:
        row = {}
        mc = vm.get('migration_considerations', {})

        # Core VM Attributes
        row['VM_Name'] = vm.get('VM', 'N/A')
        row['Power_State'] = vm.get('Powerstate', 'N/A')
        row['Is_Template'] = vm.get('is_template', False)
        row['Is_Excluded_VM_by_Pattern'] = vm.get('is_excluded_vm', False)

        # OS Information
        row['OS_RVTools_Tools_Reported'] = vm.get('OS according to the VMware Tools', 'N/A')
        row['OS_RVTools_Config_Reported'] = vm.get('OS according to the configuration file', 'N/A')
        row['OS_Primary_Source_Used'] = vm.get('OS_Primary', 'N/A')
        row['OS_Mapped_Name'] = vm.get('OS_Mapped_Name', 'N/A')
        row['OS_Complexity_Profile'] = vm.get('OS_Profile', 'N/A')

        # Resource Information
        row['CPUs_Allocated'] = vm.get('CPUs', 0)
        row['Memory_MiB_Allocated'] = vm.get('Memory', 0)
        row['Provisioned_Storage_MiB_vInfo'] = vm.get('Provisioned MiB', 0)
        row['In_Use_Storage_MiB_vInfo'] = vm.get('In Use MiB', 0)
        row['Total_Disk_Capacity_MiB_vDisk_Sum'] = vm.get('TotalDiskCapacityMiB', 0.0)
        row['Number_of_Disks'] = vm.get('num_disks', 0)
        row['Max_Disk_Size_MiB'] = vm.get('max_disk_size_mib', 0.0)

        # VM Configuration
        row['HW_Version'] = vm.get('HW version', 'N/A')
        row['Cluster'] = vm.get('Cluster', 'N/A')
        row['Resource_Pool'] = vm.get('Resource pool', 'N/A')
        row['Folder_Path'] = str(vm.get('Folder', '') or 'N/A').strip()
        row['Annotation_Purpose'] = str(vm.get('Annotation', '') or 'N/A').strip()
        row['Firmware_Type'] = str(vm.get('Firmware', 'N/A')).capitalize()
        row['VI_SDK_API_Version'] = vm.get('VI SDK API Version', 'N/A')

        # VMware Tools
        row['VMware_Tools_Status_Category'] = mc.get('tools_status_category', 'N/A')
        row['VMware_Tools_Version_Reported'] = mc.get('tools_status_version_reported', 'N/A')

        # Disk & Network Details (flattened)
        disk_controllers = sorted(list(set([str(d.get('Controller', '')).strip() for d in vm.get('DiskDetails', []) if d.get('Controller')])))
        row['Disk_Controller_Types_Present'] = ", ".join(disk_controllers) if disk_controllers else "None Reported"

        nic_adapters = sorted(list(set([str(n.get('Adapter', '')).strip() for n in vm.get('NICDetails', []) if n.get('Adapter') and n.get('Adapter').lower() != 'false'])))
        row['Network_Adapter_Types_Present'] = ", ".join(nic_adapters) if nic_adapters else "None Reported"
        row['Number_of_NICs'] = vm.get('TotalNICs', 0)


        # Migration Considerations & Risk Scoring
        row['Calculated_Risk_Score'] = mc.get('risk_score', 'N/A')
        row['Risk_Score_Reasons'] = "; ".join(mc.get('risk_score_reasons', []))

        row['Hardware_Version_Notes'] = mc.get('hw_version_notes', 'N/A')
        row['Firmware_Notes'] = mc.get('firmware_notes', 'N/A')
        row['Disk_Controller_Notes'] = mc.get('disk_controller_notes', 'N/A')
        row['Network_Adapter_Notes'] = mc.get('network_adapter_notes', 'N/A')

        row['Snapshots_Exist'] = "Yes" if vm.get('num_snapshots', 0) > 0 else "No" # From `num_snapshots` in mc, derived in `enrich_vm_data`
        row['Number_of_Snapshots'] = vm.get('num_snapshots', 0)
        row['Snapshots_Info_Notes'] = mc.get('snapshots_info', 'N/A')

        row['Passthrough_Devices_Present'] = 'Yes' if str(vm.get('Fixed Passthru HotPlug','false')).lower() == 'true' else 'No'
        row['Passthrough_Devices_Notes'] = mc.get('passthrough_notes', 'N/A')

        row['CBT_Change_Version'] = vm.get('Change Version', 'N/A')
        row['CBT_Status_Notes'] = mc.get('cbt_status_notes', 'N/A')

        row['Suspended_To_Memory_Flag'] = vm.get('Suspended To Memory', False)
        row['Suspend_Time'] = vm.get('Suspend time', 'N/A')
        row['Hibernation_Suspend_Notes'] = mc.get('hibernation_notes', 'N/A')

        row['Migration_Recommendation_Category'] = mc.get('migration_recommendation_category', 'N/A')
        row['Exclusion_Reason'] = mc.get('excluded_reason', 'N/A') # For templates/excluded VMs

        csv_data.append(row)

    if csv_data:
        df = pd.DataFrame(csv_data)
        try:
            df.to_csv(output_csv_fpath, index=False, encoding='utf-8-sig')
            print(f"CSV report successfully generated: {output_csv_fpath}")
        except Exception as e:
            print(f"Error writing CSV report to file '{output_csv_fpath}': {e}")
    else:
        print("No VM data to write to CSV.")


def main():
    parser = argparse.ArgumentParser(
        description="Analyze RVTools VM export for migration readiness, focusing on MTV compatibility aspects.",
        formatter_class=argparse.RawTextHelpFormatter, # Preserve formatting in help text
        epilog=__doc__ # Use the module docstring as epilog
    )
    parser.add_argument("rvtools_fpath", help="Path to the RVTools export CSV file (single file with multiple 'sheets').")
    parser.add_argument("-o", "--output_fpath", help="Path to save the generated TEXT report file. \nDefaults to '<rvtools_filename>__migration_focused_report.txt'.")
    parser.add_argument("-c", "--output_csv_fpath", help="Path to save the generated CSV report file (detailed per-VM). \nDefaults to '<rvtools_filename>__migration_focused_report.csv'.")
    parser.add_argument("-m", "--os_map_fpath", default=OS_BREAKDOWN_FILE_PATH,
                        help=f"Path to the OS mapping CSV file. \nDefaults to '{OS_BREAKDOWN_FILE_PATH}' in the script's directory.")
    args = parser.parse_args()

    rvtools_fpath = args.rvtools_fpath
    os_map_fpath = args.os_map_fpath

    base_name = os.path.splitext(os.path.basename(rvtools_fpath))[0]
    
    if not args.output_fpath:
        output_txt_fpath = f"{base_name}__migration_focused_report.txt"
    else:
        output_txt_fpath = args.output_fpath

    if not args.output_csv_fpath:
        output_csv_fpath = f"{base_name}__migration_focused_report.csv"
    else:
        output_csv_fpath = args.output_csv_fpath

    print(f"Processing RVTools export: {rvtools_fpath}")
    print(f"Using OS mapping CSV: {os_map_fpath}")
    print(f"Output TEXT report will be: {output_txt_fpath}")
    print(f"Output CSV report will be: {output_csv_fpath}")


    # --- Data Loading ---
    # vInfo is critical, parsed as list of dicts for detailed handling
    vinfo_list_of_dicts = read_rvtools_sheet(rvtools_fpath, 'vInfo', VINFO_COLS_ACTUAL)
    if not vinfo_list_of_dicts:
        print("Critical Error: vInfo data is empty or could not be read. This sheet is essential for the analysis. Exiting.")
        sys.exit(1)

    # Other sheets are loaded as DataFrames
    disks_df = read_rvtools_sheet(rvtools_fpath, 'vDisk', VDISK_COLS_ACTUAL)
    nics_df = read_rvtools_sheet(rvtools_fpath, 'vNetwork', VNETWORK_NIC_COLS_ACTUAL)
    snapshots_df = read_rvtools_sheet(rvtools_fpath, 'vSnapshot', VSNAPSHOT_COLS_ACTUAL)
    if snapshots_df.empty: # This is not critical but good to note
        print("Warning: vSnapshot data is empty or the sheet was not found. Snapshot detection will reflect this (no snapshots assumed).")

    vtools_df = read_rvtools_sheet(rvtools_fpath, 'vTools', VTOOLS_COLS_ACTUAL)
    if vtools_df.empty: # Not critical, but vInfo tools data will be the sole source
        print("Warning: vTools data is empty or the sheet was not found. VMware Tools information will rely solely on the vInfo sheet.")

    os_mapping = load_os_breakdown(os_map_fpath)
    if not os_mapping: # OS mapping is important for good categorization
        print(f"Warning: OS mapping data from '{os_map_fpath}' failed to load or is empty. OS categorization will be basic and may impact risk scoring accuracy.")

    # --- Data Enrichment and Analysis ---
    print("\nEnriching VM data with migration considerations...")
    enriched_vms = enrich_vm_data(vinfo_list_of_dicts, disks_df, nics_df, snapshots_df, os_mapping, vtools_df)

    if not enriched_vms:
        print("No VMs were processed after enrichment (e.g., vInfo might have been empty or contained no valid VM entries). Exiting.")
        return # Exit if no VMs to report on

    # --- Report Generation (Text and CSV) ---
    print(f"\nGenerating migration analysis reports for {len(enriched_vms)} VMs (includes templates and excluded)...")

    # Generate Text Report
    report_content_text = generate_report_text(enriched_vms, rvtools_fpath, os_map_fpath)
    try:
        with open(output_txt_fpath, 'w', encoding='utf-8') as f:
            f.write(report_content_text)
        print(f"Text report successfully generated: {output_txt_fpath}")
    except Exception as e:
        print(f"Error writing TEXT report to file '{output_txt_fpath}': {e}")

    # Generate CSV Report
    generate_csv_report(enriched_vms, output_csv_fpath)

if __name__ == "__main__":
    main()